,id,authors,publisher,link,title,summary,subjects,tasks
0,293656,"Yannick Forster,Fabian Kunze,Gert Smolka",arxiv,https://arxiv.org/abs/1806.03205,Formal Small-step Verification of a Call-by-value Lambda Calculus Machine,"We formally verify an abstract machine for a call-by-value lambda-calculuswith de Bruijn terms, simple substitution, and small-step semantics. We followa stepwise refinement approach starting with a naive stack machine withsubstitution. We then refine to a machine with closures, and finally to amachine with a heap providing structure sharing for closures. We prove thecorrectness of the three refinement steps with compositional small-stepbottom-up simulations. There is an accompanying Coq development verifying allresults.","Computer Science,Logic In Computer Science",none
1,293654,"Dennis Elbr√§chter,Philipp Grohs,Julius Berner",arxiv,https://arxiv.org/abs/1905.09803,How degenerate is the parametrization of neural networks with the ReLU activation function?,"Neural network training is usually accomplished by solving a non-convexoptimization problem using stochastic gradient descent. Although one optimizesover the networks parameters, the main loss function generally only depends onthe realization of the neural network, i.e. the function it computes. Studyingthe optimization problem over the space of realizations opens up new ways tounderstand neural network training. In particular, usual loss functions likemean squared error and categorical cross entropy are convex on spaces of neuralnetwork realizations, which themselves are non-convex. Approximationcapabilities of neural networks can be used to deal with the latternon-convexity, which allows us to establish that for sufficiently largenetworks local minima of a regularized optimization problem on the realizationspace are almost optimal. Note, however, that each realization has manydifferent, possibly degenerate, parametrizations. In particular, a localminimum in the parametrization space needs not correspond to a local minimum inthe realization space. To establish such a connection, inverse stability of therealization map is required, meaning that proximity of realizations must implyproximity of corresponding parametrizations. We present pathologies whichprevent inverse stability in general, and, for shallow networks, proceed toestablish a restricted space of parametrizations on which we have inversestability w.r.t. to a Sobolev norm. Furthermore, we show that by optimizingover such restricted sets, it is still possible to learn any function which canbe learned by optimization over unrestricted sets.","Computer Science,Machine Learning",none
2,293655,Wouter Swierstra,arxiv,https://arxiv.org/abs/1202.2924,From Mathematics to Abstract Machine: A formal derivation of an executable Krivine machine,This paper presents the derivation of an executable Krivine abstract machinefrom a small step interpreter for the simply typed lambda calculus in thedependently typed programming language Agda.,"Computer Science,Programming Languages",none
3,293653,none,aclweb,https://www.aclweb.org/anthology/W19-2003,The Influence of Down-Sampling Strategies on SVD Word Embedding Stability,"The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embedding",none,none
4,293651,"John Miller,Gabriele Vajente,Stefan Ballmer,Li Ju,Chunnong Zhao,Arnaud Pele,Koji Arai,David Blair,Dennis Coyne,Hartmut Grote,Lisa Barsotti,Robert Ward,Matthew Evans,Daniel Sigg,Hiroaki Yamamoto,Slawek Gras,Peter Fritschel,Denis Martynov,Aidan Brooks,Rich Abbott,Rana Adhikari,Rolf Bork,Bill Kells,Jameson Rollins,Nicolas Smith-Lefebvre,Ryan Derosa,Anamaria Effler,Keiko Kokeyama,Joseph Betzweiser,Valera Frolov,Adam Mullavey,Sheila Dwyer,Kiwamu Izumi,Keita Kawabe,Thomas J. Massinger,Alexa Staley,Chris Mueller,Eleanor King,Keith Thorne,Michael Thomas,Carl Adams,Stuart Aston,Janeen Romie,Matthew Heinze",arxiv,https://arxiv.org/abs/1502.06058,Observation of Parametric Instability in Advanced LIGO,"Parametric instabilities have long been studied as a potentially limitingeffect in high-power interferometric gravitational wave detectors. Until now,however, these instabilities have never been observed in a kilometer-scaleinterferometer. In this work we describe the first observation of parametricinstability in an Advanced LIGO detector, and the means by which it has beenremoved as a barrier to progress.","Astrophysics,Instrumentation And Methods For Astrophysics",none
