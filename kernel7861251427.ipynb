{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":5,"outputs":[{"output_type":"stream","text":"/kaggle/input/bertbase/bert-base-uncased-vocab.txt\n/kaggle/input/recsys/suggest_dump.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.randn((200000,100)).cuda()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = []\ncos=nn.CosineSimilarity(dim=0).cuda()\ni=a[1]\nc=[]\nfor j in tqdm.tqdm(a):\n    c.append(cos(i.view(-1,1),j.view(-1,1)).tolist())\nb.append(c)    ","execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tqdm' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-fcefb55451d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.topk(torch.tensor(b).view(1,-1).cuda(),k=10).indices.tolist()","execution_count":9,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"invalid argument 5: k not in range for dimension at /opt/conda/conda-bld/pytorch_1591914880026/work/aten/src/THC/generic/THCTensorTopK.cu:23","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-3f0db46a1199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: invalid argument 5: k not in range for dimension at /opt/conda/conda-bld/pytorch_1591914880026/work/aten/src/THC/generic/THCTensorTopK.cu:23"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/recsys/suggest_dump.txt',delimiter='\\t')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id']=1","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shortuuid\nimport tqdm","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[1,'id']","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm(range(len(train))):\n    train.loc[i,'id']=shortuuid.uuid()","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|██████████| 197465/197465 [00:37<00:00, 5214.80it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"                                            authors publisher  \\\n0                   Jagadeesan Jayender,Haoyin Zhou     arxiv   \n1   Xu Chen,Lin Chen,Xin Tang,Shuai Yu,Liekang Zeng     arxiv   \n2     Huchuan Lu,Lihe Zhang,Youwei Pang,Xiaoqi Zhao     arxiv   \n3    Parth Patel,Kalpdrum Passi,Chakresh Kumar Jain     arxiv   \n4  Danail Stoyanov,Emanuele Colleoni,Philip Edwards     arxiv   \n\n                                 link  \\\n0  https://arxiv.org/abs/2007.08553v1   \n1  https://arxiv.org/abs/2007.09072v1   \n2  https://arxiv.org/abs/2007.09062v1   \n3  https://arxiv.org/abs/2007.08652v1   \n4  https://arxiv.org/abs/2007.09107v1   \n\n                                               title  \\\n0  Smooth Deformation Field-based Mismatch Remova...   \n1  Joint Multi-User DNN Partitioning and Computat...   \n2  Multi-scale Interactive Network for Salient Ob...   \n3  Prediction of Cancer Microarray and DNA Methyl...   \n4  Synthetic and Real Inputs for ToolSegmentation...   \n\n                                             summary  \\\n0        Abstract:  This paper studies the mismat...   \n1        Abstract:  Mobile Edge Computing (MEC) h...   \n2        Abstract:  Deep-learning based salient o...   \n3        Abstract:  Over the past few years, ther...   \n4        Abstract:  Semantic tool segmentation in...   \n\n                                            subjects  \\\n0  Computer Science,Computer Vision And Pattern R...   \n1  Computer Science,Distributed, Parallel, And Cl...   \n2  Computer Science,Computer Vision And Pattern R...   \n3          Quantitative Biology,Quantitative Methods   \n4  Computer Science,Computer Vision And Pattern R...   \n\n                                       tasks        year  \\\n0                                        NaN  2020-20-20   \n1                                        NaN  2020-20-20   \n2  Object Detection,Salient Object Detection  2020-20-20   \n3                                        NaN  2020-20-20   \n4                        Scene Understanding  2020-20-20   \n\n                       id  \n0  SNqRRH9FnjbcvfLse3DpfJ  \n1  Yd5KdsYkSPTpi6ycCoctPE  \n2  K5vxRZ7rkVSyzrLymDgc2V  \n3  MJuwaV7GX83Qcfn4Dhkk6E  \n4  axkLK5TUkMEvwxWEJQhWBw  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authors</th>\n      <th>publisher</th>\n      <th>link</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>subjects</th>\n      <th>tasks</th>\n      <th>year</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jagadeesan Jayender,Haoyin Zhou</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.08553v1</td>\n      <td>Smooth Deformation Field-based Mismatch Remova...</td>\n      <td>Abstract:  This paper studies the mismat...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>SNqRRH9FnjbcvfLse3DpfJ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Xu Chen,Lin Chen,Xin Tang,Shuai Yu,Liekang Zeng</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09072v1</td>\n      <td>Joint Multi-User DNN Partitioning and Computat...</td>\n      <td>Abstract:  Mobile Edge Computing (MEC) h...</td>\n      <td>Computer Science,Distributed, Parallel, And Cl...</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>Yd5KdsYkSPTpi6ycCoctPE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Huchuan Lu,Lihe Zhang,Youwei Pang,Xiaoqi Zhao</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09062v1</td>\n      <td>Multi-scale Interactive Network for Salient Ob...</td>\n      <td>Abstract:  Deep-learning based salient o...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>Object Detection,Salient Object Detection</td>\n      <td>2020-20-20</td>\n      <td>K5vxRZ7rkVSyzrLymDgc2V</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Parth Patel,Kalpdrum Passi,Chakresh Kumar Jain</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.08652v1</td>\n      <td>Prediction of Cancer Microarray and DNA Methyl...</td>\n      <td>Abstract:  Over the past few years, ther...</td>\n      <td>Quantitative Biology,Quantitative Methods</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>MJuwaV7GX83Qcfn4Dhkk6E</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Danail Stoyanov,Emanuele Colleoni,Philip Edwards</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09107v1</td>\n      <td>Synthetic and Real Inputs for ToolSegmentation...</td>\n      <td>Abstract:  Semantic tool segmentation in...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>Scene Understanding</td>\n      <td>2020-20-20</td>\n      <td>axkLK5TUkMEvwxWEJQhWBw</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = train['authors'].fillna('')+' '+train['title'].fillna('')+' '+train['summary'].fillna('')+' '+train['subjects'].fillna('')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.index = train['id']\ncorpus.name = 'text'","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm(range(len(corpus)//10000+1)):\n    corpus[10000*i:10000*(i+1)].to_csv('corpus{}.csv'.format(i))","execution_count":18,"outputs":[{"output_type":"stream","text":"100%|██████████| 20/20 [00:10<00:00,  2.00it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import Field,RawField, TabularDataset","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\nimport os\nimport re\nimport signal\nfrom math import ceil\nfrom os.path import join\nfrom time import time\nimport numpy as np\nimport torch\nfrom numpy.random import choice\nfrom torchtext.data import RawField,Field, TabularDataset\nfrom spacy.lang.en import STOP_WORDS\nimport string\n\nclass DataMaker():\n    def __init__(self, datapath, batch_size, context_size,\n                 num_noise_words=20, max_size=0, num_workers=None):\n        self.datapath=datapath\n        self.batch_size = batch_size\n        self.context_size = context_size\n        self.num_noise_words = num_noise_words\n        self.max_size = max_size\n        self.num_workers = num_workers\n        self.stopwords = list(STOP_WORDS)+list((''.join(string.punctuation)).strip(''))+['-pron-','-PRON-']\n        self.dataset = None\n        \n    def _tokenize_str(self,str_):\n        # keep only alphanumeric and punctations\n        str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n        # remove multiple whitespace characters\n        str_ = re.sub(r'\\s{2,}', ' ', str_)\n        # punctations to tokens\n        str_ = re.sub(r'\\(', ' ( ', str_)\n        str_ = re.sub(r'\\)', ' ) ', str_)\n        str_ = re.sub(r',', ' , ', str_)\n        str_ = re.sub(r'\\.', ' . ', str_)\n        str_ = re.sub(r'!', ' ! ', str_)\n        str_ = re.sub(r'\\?', ' ? ', str_)\n        # split contractions into multiple tokens\n        str_ = re.sub(r'\\'s', ' \\'s', str_)\n        str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n        str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n        str_ = re.sub(r'\\'re', ' \\'re', str_)\n        str_ = re.sub(r'\\'d', ' \\'d', str_)\n        str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n        # lower case\n        return str_.strip().lower().split()\n    \n    def getdata(self):\n        start_time=time()\n        doc_field = RawField()\n        text_field = Field(stop_words=self.stopwords,pad_token=None,tokenize=self._tokenize_str)\n        dataset = TabularDataset(self.datapath,skip_header=True,format='CSV',fields=[('id',doc_field),('text',text_field)])\n        text_field.build_vocab(dataset)\n        self.dataset=dataset\n        print('Time taken --- {} seconds'.format(time()-start_time))\n        return dataset\n    \n    def getgenerator(self):\n        return NCEData(self.dataset,self.batch_size,self.context_size,self.num_noise_words,self.max_size,self.num_workers)\n\n\nclass NCEData(object):\n    \n    # code inspired by parallel generators in https://github.com/fchollet/keras\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, max_size, num_workers):\n        self.max_size = max_size\n\n        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()\n        if self.num_workers is None:\n            self.num_workers = 1\n\n        self._generator = _NCEGenerator(\n            dataset,\n            batch_size,\n            context_size,\n            num_noise_words,\n            _NCEGeneratorState(context_size))\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def __len__(self):\n        return len(self._generator)\n\n    def vocabulary_size(self):\n        return self._generator.vocabulary_size()\n\n    def start(self):\n        \"\"\"Starts num_worker processes that generate batches of data.\"\"\"\n        self._queue = multiprocessing.Queue(maxsize=self.max_size)\n        self._stop_event = multiprocessing.Event()\n\n        for _ in range(self.num_workers):\n            process = multiprocessing.Process(target=self._parallel_task)\n            process.daemon = True\n            self._processes.append(process)\n            process.start()\n\n    def _parallel_task(self):\n        while not self._stop_event.is_set():\n            try:\n                batch = self._generator.next()\n                # queue blocks a call to put() until a free slot is available\n                self._queue.put(batch)\n            except KeyboardInterrupt:\n                self._stop_event.set()\n\n    def get_generator(self):\n        \"\"\"Returns a generator that yields batches of data.\"\"\"\n        while self._is_running():\n            yield self._queue.get()\n\n    def stop(self):\n        \"\"\"Terminates all processes that were created with start().\"\"\"\n        if self._is_running():\n            self._stop_event.set()\n\n        for process in self._processes:\n            if process.is_alive():\n                os.kill(process.pid, signal.SIGINT)\n                process.join()\n\n        if self._queue is not None:\n            self._queue.close()\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def _is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n\nclass _NCEGenerator(object):\n    \"\"\"An infinite, process-safe batch generator for noise-contrastive\n    estimation of word vector models.\n    Parameters\n    ----------\n    state: paragraphvec.data._NCEGeneratorState\n        Initial (indexing) state of the generator.\n    For other parameters see the NCEData class.\n    \"\"\"\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, state):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.context_size = context_size\n        self.num_noise_words = num_noise_words\n\n        self._vocabulary = self.dataset.fields['text'].vocab\n        self._sample_noise = None\n        self._init_noise_distribution()\n        self._state = state\n\n    def _init_noise_distribution(self):\n        # we use a unigram distribution raised to the 3/4rd power,\n        # as proposed by T. Mikolov et al. in Distributed Representations\n        # of Words and Phrases and their Compositionality\n        probs = np.zeros(len(self._vocabulary) - 1)\n\n        for word, freq in self._vocabulary.freqs.items():\n            probs[self._word_to_index(word)] = freq\n\n        probs = np.power(probs, 0.75)\n        probs /= np.sum(probs)\n\n        self._sample_noise = lambda: choice(\n            probs.shape[0], self.num_noise_words, p=probs).tolist()\n\n    def __len__(self):\n        num_examples = sum(self._num_examples_in_doc(d) for d in self.dataset)\n        return ceil(num_examples / self.batch_size)\n\n    def vocabulary_size(self):\n        return len(self._vocabulary) - 1\n\n    def next(self):\n        \"\"\"Updates state for the next process in a process-safe manner\n        and generates the current batch.\"\"\"\n        prev_doc_id, prev_in_doc_pos = self._state.update_state(\n            self.dataset,\n            self.batch_size,\n            self.context_size,\n            self._num_examples_in_doc)\n\n        # generate the actual batch\n        batch = _NCEBatch(self.context_size)\n\n        while len(batch) < self.batch_size:\n            if prev_doc_id == len(self.dataset):\n                # last document exhausted\n                batch.torch_()\n                return batch\n            if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n                                   - self.context_size):\n                # more examples in the current document\n                self._add_example_to_batch(prev_doc_id, prev_in_doc_pos, batch)\n                prev_in_doc_pos += 1\n            else:\n                # go to the next document\n                prev_doc_id += 1\n                prev_in_doc_pos = self.context_size\n\n        batch.torch_()\n        return batch\n\n    def _num_examples_in_doc(self, doc, in_doc_pos=None):\n        if in_doc_pos is not None:\n            # number of remaining\n            if len(doc.text) - in_doc_pos >= self.context_size + 1:\n                return len(doc.text) - in_doc_pos - self.context_size\n            return 0\n\n        if len(doc.text) >= 2 * self.context_size + 1:\n            # total number\n            return len(doc.text) - 2 * self.context_size\n        return 0\n\n    def _add_example_to_batch(self, doc_id, in_doc_pos, batch):\n        doc = self.dataset[doc_id].text\n        batch.doc_ids.append(doc_id)\n\n        # sample from the noise distribution\n        current_noise = self._sample_noise()\n        current_noise.insert(0, self._word_to_index(doc[in_doc_pos]))\n        batch.target_noise_ids.append(current_noise)\n\n        if self.context_size == 0:\n            return\n\n        current_context = []\n        context_indices = (in_doc_pos + diff for diff in\n                           range(-self.context_size, self.context_size + 1)\n                           if diff != 0)\n\n        for i in context_indices:\n            context_id = self._word_to_index(doc[i])\n            current_context.append(context_id)\n        batch.context_ids.append(current_context)\n\n    def _word_to_index(self, word):\n        return self._vocabulary.stoi[word] - 1\n\n\nclass _NCEGeneratorState(object):\n    \"\"\"Batch generator state that is represented with a document id and\n    in-document position. It abstracts a process-safe indexing mechanism.\"\"\"\n    def __init__(self, context_size):\n        # use raw values because both indices have\n        # to manually be locked together\n        self._doc_id = multiprocessing.RawValue('i', 0)\n        self._in_doc_pos = multiprocessing.RawValue('i', context_size)\n        self._lock = multiprocessing.Lock()\n\n    def update_state(self, dataset, batch_size,\n                     context_size, num_examples_in_doc):\n        \"\"\"Returns current indices and computes new indices for the\n        next process.\"\"\"\n        with self._lock:\n            doc_id = self._doc_id.value\n            in_doc_pos = self._in_doc_pos.value\n            self._advance_indices(\n                dataset, batch_size, context_size, num_examples_in_doc)\n            return doc_id, in_doc_pos\n\n    def _advance_indices(self, dataset, batch_size,\n                         context_size, num_examples_in_doc):\n        num_examples = num_examples_in_doc(\n            dataset[self._doc_id.value], self._in_doc_pos.value)\n\n        if num_examples > batch_size:\n            # more examples in the current document\n            self._in_doc_pos.value += batch_size\n            return\n\n        if num_examples == batch_size:\n            # just enough examples in the current document\n            if self._doc_id.value < len(dataset) - 1:\n                self._doc_id.value += 1\n            else:\n                self._doc_id.value = 0\n            self._in_doc_pos.value = context_size\n            return\n\n        while num_examples < batch_size:\n            if self._doc_id.value == len(dataset) - 1:\n                # last document: reset indices\n                self._doc_id.value = 0\n                self._in_doc_pos.value = context_size\n                return\n\n            self._doc_id.value += 1\n            num_examples += num_examples_in_doc(\n                dataset[self._doc_id.value])\n\n        self._in_doc_pos.value = (len(dataset[self._doc_id.value].text)\n                                  - context_size\n                                  - (num_examples - batch_size))\n\n\nclass _NCEBatch(object):\n    def __init__(self, context_size):\n        self.context_ids = [] if context_size > 0 else None\n        self.doc_ids = []\n        self.target_noise_ids = []\n\n    def __len__(self):\n        return len(self.doc_ids)\n\n    def torch_(self):\n        if self.context_ids is not None:\n            self.context_ids = torch.LongTensor(self.context_ids)\n        self.doc_ids = torch.LongTensor(self.doc_ids)\n        self.target_noise_ids = torch.LongTensor(self.target_noise_ids)\n\n    def cuda_(self):\n        if self.context_ids is not None:\n            self.context_ids = self.context_ids.cuda()\n        self.doc_ids = self.doc_ids.cuda()\n        self.target_noise_ids = self.target_noise_ids.cuda()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataMaker('corpus1.csv',batch_size=256,context_size=3,num_workers=-1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=data.getdata()","execution_count":23,"outputs":[{"output_type":"stream","text":"Time taken --- 2.756767988204956 seconds\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = data.getgenerator()","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(generator)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"3636"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NegativeSampling(nn.Module):\n    \"\"\"Negative sampling loss as proposed by T. Mikolov et al. in Distributed\n    Representations of Words and Phrases and their Compositionality.\n    \"\"\"\n    def __init__(self):\n        super(NegativeSampling, self).__init__()\n        self._log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, scores):\n        \"\"\"Computes the value of the loss function.\n        Parameters\n        ----------\n        scores: autograd.Variable of size (batch_size, num_noise_words + 1)\n            Sparse unnormalized log probabilities. The first element in each\n            row is the ground truth score (i.e. the target), other elements\n            are scores of samples from the noise distribution.\n        \"\"\"\n        k = scores.size()[1] - 1\n        return -torch.sum(\n            self._log_sigmoid(scores[:, 0])\n            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n        ) / scores.size()[0]","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DM(nn.Module):\n    \"\"\"Distributed Memory version of Paragraph Vectors.\n    Parameters\n    ----------\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n    num_docs: int\n        Number of documents in a dataset.\n    num_words: int\n        Number of distinct words in a daset (i.e. vocabulary size).\n    \"\"\"\n    def __init__(self, vec_dim, num_docs, num_words):\n        super(DM, self).__init__()\n        # paragraph matrix\n        self._D = nn.Parameter(\n            torch.randn(num_docs, vec_dim), requires_grad=True)\n        # word matrix\n        self._W = nn.Parameter(\n            torch.randn(num_words, vec_dim), requires_grad=True)\n        # output layer parameters\n        self._O = nn.Parameter(\n            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n\n    def forward(self, context_ids, doc_ids, target_noise_ids):\n        \"\"\"Sparse computation of scores (unnormalized log probabilities)\n        that should be passed to the negative sampling loss.\n        Parameters\n        ----------\n        context_ids: torch.Tensor of size (batch_size, num_context_words)\n            Vocabulary indices of context words.\n        doc_ids: torch.Tensor of size (batch_size,)\n            Document indices of paragraphs.\n        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n            Vocabulary indices of target and noise words. The first element in\n            each row is the ground truth index (i.e. the target), other\n            elements are indices of samples from the noise distribution.\n        Returns\n        -------\n            autograd.Variable of size (batch_size, num_noise_words + 1)\n        \"\"\"\n        # combine a paragraph vector with word vectors of\n        # input (context) words\n        x = torch.add(\n            self._D[doc_ids, :], torch.sum(self._W[context_ids, :], dim=1))\n\n        # sparse computation of scores (unnormalized log probabilities)\n        # for negative sampling\n        return torch.bmm(\n            x.unsqueeze(1),\n            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n\n    def get_paragraph_vector(self):\n        return self._D.data.tolist()\n    \n    def fit(self,generator,epochs,num_batches):\n        \n        opt=torch.optim.Adam(self.parameters(),lr=0.0001)\n        cost_func = NegativeSampling()\n        if torch.cuda.is_available():            \n            cost_func.cuda()\n\n        generator.start()        \n        \n        batch_maker = generator.get_generator()\n        for epoch in range(epochs):\n            step = 0\n            loss=[]\n            for batch_i in tqdm.tqdm(range(num_batches)):\n#                 step+=1\n                batch = next(batch_maker)\n                if torch.cuda.is_available():\n                    batch.cuda_()\n                \n                x = self.forward(\n                        batch.context_ids,\n                        batch.doc_ids,\n                        batch.target_noise_ids) \n                x = cost_func.forward(x)\n                loss.append(x.item())\n                self.zero_grad()\n                x.backward()\n                opt.step()\n#                 if step%100==0:\n#                     print('-',end='')\n            loss = torch.mean(torch.FloatTensor(loss))\n            print('epoch - {} loss - {:.4f}'.format(epoch+1,loss))\n        generator.stop()\n        \n    def get_embeddings(self,ids):\n        docvecs = self._D.data.tolist()\n        if len(docvecs)!=len(ids):\n            raise(\"Length of ids does'nt match\")\n            return 0\n        self.embeddings = pd.DataFrame({'ids':ids,'vector':docvecs})\n        return self.embeddings\n    \n    def most_similar_docs(self,docs,topk=11):\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        if not isinstance(docs,list):\n            docs = [docs]\n            \n        if not self.embeddings.ids.isin(docs).any():\n            raise('Not in vocab')\n            return 0\n        \n        index = self.embeddings['vector'][self.embeddings['ids'].isin(docs)].values.tolist()\n        index = torch.LongTensor(index).to(device)\n        embeddings = torch.FloatTensor(self.embeddings['vector'].values.tolist()).to(device)\n        similars = self._similarity(index,embeddings,topk)\n        similar_docs = self.embeddings['ids'][similars.indices.tolist()[0]].values.tolist()\n        probs = similars.values.tolist()[0]\n        \n        return similar_docs[1:], probs[1:]\n        \n    def _similarity(self,doc,embeddings,topk):\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        similarity = []\n        \n        cos=nn.CosineSimilarity(dim=0).to(device)\n        for i in doc:\n            inner = []\n            for j in embeddings:\n                inner.append(cos(i.view(-1,1),j.view(-1,1)).tolist())\n            similarity.append(inner)\n        similarity = torch.FloatTensor(similarity).view(1,-1).to(device)\n        return torch.topk(similarity,topk)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DM(vec_dim=100,num_docs=len(dataset),num_words=generator.vocabulary_size()).cuda()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.vocabulary_size()","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"107701"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(generator)","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"3636"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(generator,num_batches=len(generator),epochs=5)","execution_count":31,"outputs":[{"output_type":"stream","text":"100%|██████████| 3636/3636 [11:59<00:00,  5.05it/s]\n  0%|          | 0/3636 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch - 1 loss - 1.3200\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3636/3636 [12:05<00:00,  5.01it/s]\n  0%|          | 0/3636 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch - 2 loss - 1.1657\n","name":"stdout"},{"output_type":"stream","text":" 23%|██▎       | 830/3636 [02:45<09:19,  5.02it/s]\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-c713c88ad0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-ab1d459b80ec>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, generator, epochs, num_batches)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#                 step+=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_maker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-05209940aaf2>\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"Returns a generator that yields batches of data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"getids = pd.read_csv('corpus1.csv')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getids.id","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"0       282JrhE8B9dJN8sJF3TEzE\n1       Ff3j8fApeY7iFavuCDfsKS\n2       7cNmGgcR2eZ2gB6yB3kh7r\n3       QkQvYBXXXtKWhtAQNf8UFn\n4       fmherzLQ7FYVUCvrpeLBbT\n                 ...          \n9995    3xNSwsDFzA7smPvzhzFVVQ\n9996    HLHECNEA8i4u6TAdCSjYaZ\n9997    YBr8yMHuew7VYvxYYNSgmr\n9998    JGC6izRWCA8rwfTaffbFvt\n9999    UmKbFbBhcGy2q7u92BrYJy\nName: id, Length: 10000, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = model.get_embeddings(getids['id'].values.tolist())","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim = model.most_similar_docs(['YBr8yMHuew7VYvxYYNSgmr'])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['id'].isin(sim[0]+['YBr8yMHuew7VYvxYYNSgmr'])]","execution_count":166,"outputs":[{"output_type":"execute_result","execution_count":166,"data":{"text/plain":"                                                authors publisher  \\\n1032                   Xu Wang,Mladen Kolar,Ali Shojaie     arxiv   \n1060                 Cheng Wang,Zhou Tang,Zhangsheng Yu     arxiv   \n1122  Won-Ki Jeong,Tran Minh Quan,Tuan Tran Anh,Khoa...     arxiv   \n1201  Anima Anandkumar,Wuyang Chen,Zhangyang Wang,Zh...     arxiv   \n1216  Xiang Yue,Feng Huang,Wen Zhang,Zhankun Xiong,Z...     arxiv   \n1325   Matthias Schubert,Michael Shekelyan,Gregor Jossé     arxiv   \n1527                          Dongming Yang,YueXian Zou     arxiv   \n1811  Lucas Gabriel Souza França,Pedro Montoya,José ...     arxiv   \n1971       Christopher De Sa,Karen Levy,A. Feder Cooper     arxiv   \n1988                 Clayton G. Webster,Joseph Daws Jr.     arxiv   \n1998        Wei Zhang,Tao Mei,Yalong Bai,Hongdong Zheng     arxiv   \n\n                                    link  \\\n1032  https://arxiv.org/abs/2007.07448v1   \n1060    https://arxiv.org/abs/1901.07150   \n1122    https://arxiv.org/abs/2005.07058   \n1201    https://arxiv.org/abs/2007.06965   \n1216    https://arxiv.org/abs/1911.05584   \n1325     https://arxiv.org/abs/1410.0205   \n1527  https://arxiv.org/abs/2007.06925v1   \n1811    https://arxiv.org/abs/1702.03912   \n1971    https://arxiv.org/abs/2007.02203   \n1988    https://arxiv.org/abs/1905.10457   \n1998    https://arxiv.org/abs/1909.00640   \n\n                                                  title  \\\n1032  Statistical Inference for Networks of High-Dim...   \n1060  A Fast Iterative Algorithm for High-dimensiona...   \n1122  Reinforced Coloring for End-to-End Instance Se...   \n1201         Automated Synthetic-to-Real Generalization   \n1216  Tensor Decomposition with Relational Constrain...   \n1325  ParetoPrep: Fast computation of Path Skylines ...   \n1527  A Graph-based Interactive Reasoning for Human-...   \n1811  On multifractals: a non-linear study of actigr...   \n1971  Regulating Accuracy-Efficiency Trade-Offs in D...   \n1988  A Polynomial-Based Approach for Architectural ...   \n1998  Relationship-Aware Spatial Perception Fusion f...   \n\n                                                summary  \\\n1032          Fueled in part by recent applications ...   \n1060  Differential network is an important tool to c...   \n1122  Instance segmentation is one of the actively s...   \n1201  Models trained on synthetic images often face ...   \n1216  MicroRNAs (miRNAs) play crucial roles in multi...   \n1325  Computing cost optimal paths in network data i...   \n1527          Human-Object Interaction (HOI) detecti...   \n1811  This work aimed, to determine the characterist...   \n1971  In this paper we discuss the trade-off between...   \n1988  In this effort we propose a novel approach for...   \n1998  The significant progress on Generative Adversa...   \n\n                                               subjects  \\\n1032                        Machine Learning,Statistics   \n1060                             Statistics,Computation   \n1122  Computer Science,Computer Vision And Pattern R...   \n1201                  Computer Science,Machine Learning   \n1216                  Computer Science,Machine Learning   \n1325                         Computer Science,Databases   \n1527  Computer Science,Computer Vision And Pattern R...   \n1811                Nonlinear Sciences,Chaotic Dynamics   \n1971             Computer Science,Computers And Society   \n1988                  Computer Science,Machine Learning   \n1998  Computer Science,Computer Vision And Pattern R...   \n\n                                   tasks        year                      id  \n1032                     Point Processes  2020-20-20  gMAoeMzckx75dFpRexijWa  \n1060                                 NaN  2019-19-19  Uvq2FjtCnnQuL39hAt4whM  \n1122                                 NaN  2020-20-20  2xWqvsNBjphZa6wKPbpU2S  \n1201                                 NaN  2020-20-20  k3QFPSXqXUdSxJGoJKt2VY  \n1216                                 NaN  2019-19-19  jExxTwPuvNQQkv75Bn5MaC  \n1325                                 NaN  2014-14-14  kBjkemWn6XAAGoojSb82HT  \n1527  Human-Object Interaction Detection  2020-20-20  64J4DhtLCB5rExACY8uJtV  \n1811                                 NaN  2017-17-17  FGEtG7cJozuP7fVn4hLfuV  \n1971                                 NaN  2020-20-20  8WjtQprVejGDZB3Asay2We  \n1988                                 NaN  2019-19-19  LspcZfnPuqpmf2UpE5bskA  \n1998                                 NaN  2019-19-19  FCxD4vvhCTLFzGnYG2kFJC  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authors</th>\n      <th>publisher</th>\n      <th>link</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>subjects</th>\n      <th>tasks</th>\n      <th>year</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1032</th>\n      <td>Xu Wang,Mladen Kolar,Ali Shojaie</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.07448v1</td>\n      <td>Statistical Inference for Networks of High-Dim...</td>\n      <td>Fueled in part by recent applications ...</td>\n      <td>Machine Learning,Statistics</td>\n      <td>Point Processes</td>\n      <td>2020-20-20</td>\n      <td>gMAoeMzckx75dFpRexijWa</td>\n    </tr>\n    <tr>\n      <th>1060</th>\n      <td>Cheng Wang,Zhou Tang,Zhangsheng Yu</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1901.07150</td>\n      <td>A Fast Iterative Algorithm for High-dimensiona...</td>\n      <td>Differential network is an important tool to c...</td>\n      <td>Statistics,Computation</td>\n      <td>NaN</td>\n      <td>2019-19-19</td>\n      <td>Uvq2FjtCnnQuL39hAt4whM</td>\n    </tr>\n    <tr>\n      <th>1122</th>\n      <td>Won-Ki Jeong,Tran Minh Quan,Tuan Tran Anh,Khoa...</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2005.07058</td>\n      <td>Reinforced Coloring for End-to-End Instance Se...</td>\n      <td>Instance segmentation is one of the actively s...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>2xWqvsNBjphZa6wKPbpU2S</td>\n    </tr>\n    <tr>\n      <th>1201</th>\n      <td>Anima Anandkumar,Wuyang Chen,Zhangyang Wang,Zh...</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.06965</td>\n      <td>Automated Synthetic-to-Real Generalization</td>\n      <td>Models trained on synthetic images often face ...</td>\n      <td>Computer Science,Machine Learning</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>k3QFPSXqXUdSxJGoJKt2VY</td>\n    </tr>\n    <tr>\n      <th>1216</th>\n      <td>Xiang Yue,Feng Huang,Wen Zhang,Zhankun Xiong,Z...</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1911.05584</td>\n      <td>Tensor Decomposition with Relational Constrain...</td>\n      <td>MicroRNAs (miRNAs) play crucial roles in multi...</td>\n      <td>Computer Science,Machine Learning</td>\n      <td>NaN</td>\n      <td>2019-19-19</td>\n      <td>jExxTwPuvNQQkv75Bn5MaC</td>\n    </tr>\n    <tr>\n      <th>1325</th>\n      <td>Matthias Schubert,Michael Shekelyan,Gregor Jossé</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1410.0205</td>\n      <td>ParetoPrep: Fast computation of Path Skylines ...</td>\n      <td>Computing cost optimal paths in network data i...</td>\n      <td>Computer Science,Databases</td>\n      <td>NaN</td>\n      <td>2014-14-14</td>\n      <td>kBjkemWn6XAAGoojSb82HT</td>\n    </tr>\n    <tr>\n      <th>1527</th>\n      <td>Dongming Yang,YueXian Zou</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.06925v1</td>\n      <td>A Graph-based Interactive Reasoning for Human-...</td>\n      <td>Human-Object Interaction (HOI) detecti...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>Human-Object Interaction Detection</td>\n      <td>2020-20-20</td>\n      <td>64J4DhtLCB5rExACY8uJtV</td>\n    </tr>\n    <tr>\n      <th>1811</th>\n      <td>Lucas Gabriel Souza França,Pedro Montoya,José ...</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1702.03912</td>\n      <td>On multifractals: a non-linear study of actigr...</td>\n      <td>This work aimed, to determine the characterist...</td>\n      <td>Nonlinear Sciences,Chaotic Dynamics</td>\n      <td>NaN</td>\n      <td>2017-17-17</td>\n      <td>FGEtG7cJozuP7fVn4hLfuV</td>\n    </tr>\n    <tr>\n      <th>1971</th>\n      <td>Christopher De Sa,Karen Levy,A. Feder Cooper</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.02203</td>\n      <td>Regulating Accuracy-Efficiency Trade-Offs in D...</td>\n      <td>In this paper we discuss the trade-off between...</td>\n      <td>Computer Science,Computers And Society</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>8WjtQprVejGDZB3Asay2We</td>\n    </tr>\n    <tr>\n      <th>1988</th>\n      <td>Clayton G. Webster,Joseph Daws Jr.</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1905.10457</td>\n      <td>A Polynomial-Based Approach for Architectural ...</td>\n      <td>In this effort we propose a novel approach for...</td>\n      <td>Computer Science,Machine Learning</td>\n      <td>NaN</td>\n      <td>2019-19-19</td>\n      <td>LspcZfnPuqpmf2UpE5bskA</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>Wei Zhang,Tao Mei,Yalong Bai,Hongdong Zheng</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/1909.00640</td>\n      <td>Relationship-Aware Spatial Perception Fusion f...</td>\n      <td>The significant progress on Generative Adversa...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>NaN</td>\n      <td>2019-19-19</td>\n      <td>FCxD4vvhCTLFzGnYG2kFJC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}