{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/recsys/suggest_dump.txt\n/kaggle/input/bertbase/bert-base-uncased-vocab.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/recsys/suggest_dump.txt',delimiter='\\t')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id']=1","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shortuuid\nimport tqdm","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[1,'id']","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm(range(len(train))):\n    train.loc[i,'id']=shortuuid.uuid()","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 197465/197465 [00:43<00:00, 4528.72it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"                                            authors publisher  \\\n0                   Jagadeesan Jayender,Haoyin Zhou     arxiv   \n1   Xu Chen,Lin Chen,Xin Tang,Shuai Yu,Liekang Zeng     arxiv   \n2     Huchuan Lu,Lihe Zhang,Youwei Pang,Xiaoqi Zhao     arxiv   \n3    Parth Patel,Kalpdrum Passi,Chakresh Kumar Jain     arxiv   \n4  Danail Stoyanov,Emanuele Colleoni,Philip Edwards     arxiv   \n\n                                 link  \\\n0  https://arxiv.org/abs/2007.08553v1   \n1  https://arxiv.org/abs/2007.09072v1   \n2  https://arxiv.org/abs/2007.09062v1   \n3  https://arxiv.org/abs/2007.08652v1   \n4  https://arxiv.org/abs/2007.09107v1   \n\n                                               title  \\\n0  Smooth Deformation Field-based Mismatch Remova...   \n1  Joint Multi-User DNN Partitioning and Computat...   \n2  Multi-scale Interactive Network for Salient Ob...   \n3  Prediction of Cancer Microarray and DNA Methyl...   \n4  Synthetic and Real Inputs for ToolSegmentation...   \n\n                                             summary  \\\n0        Abstract:  This paper studies the mismat...   \n1        Abstract:  Mobile Edge Computing (MEC) h...   \n2        Abstract:  Deep-learning based salient o...   \n3        Abstract:  Over the past few years, ther...   \n4        Abstract:  Semantic tool segmentation in...   \n\n                                            subjects  \\\n0  Computer Science,Computer Vision And Pattern R...   \n1  Computer Science,Distributed, Parallel, And Cl...   \n2  Computer Science,Computer Vision And Pattern R...   \n3          Quantitative Biology,Quantitative Methods   \n4  Computer Science,Computer Vision And Pattern R...   \n\n                                       tasks        year  \\\n0                                        NaN  2020-20-20   \n1                                        NaN  2020-20-20   \n2  Object Detection,Salient Object Detection  2020-20-20   \n3                                        NaN  2020-20-20   \n4                        Scene Understanding  2020-20-20   \n\n                       id  \n0  nDEUmZdG3aserLKV39Lgnk  \n1  exHSkPgPTVDX5LHWrMTLUS  \n2  WERC2FHVGJZrzy3MJHGziD  \n3  PbLkrR96b86mH4LgQKFrQ9  \n4  9VDuxxjMfB8oHUut7rBaWL  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authors</th>\n      <th>publisher</th>\n      <th>link</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>subjects</th>\n      <th>tasks</th>\n      <th>year</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jagadeesan Jayender,Haoyin Zhou</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.08553v1</td>\n      <td>Smooth Deformation Field-based Mismatch Remova...</td>\n      <td>Abstract:  This paper studies the mismat...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>nDEUmZdG3aserLKV39Lgnk</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Xu Chen,Lin Chen,Xin Tang,Shuai Yu,Liekang Zeng</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09072v1</td>\n      <td>Joint Multi-User DNN Partitioning and Computat...</td>\n      <td>Abstract:  Mobile Edge Computing (MEC) h...</td>\n      <td>Computer Science,Distributed, Parallel, And Cl...</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>exHSkPgPTVDX5LHWrMTLUS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Huchuan Lu,Lihe Zhang,Youwei Pang,Xiaoqi Zhao</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09062v1</td>\n      <td>Multi-scale Interactive Network for Salient Ob...</td>\n      <td>Abstract:  Deep-learning based salient o...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>Object Detection,Salient Object Detection</td>\n      <td>2020-20-20</td>\n      <td>WERC2FHVGJZrzy3MJHGziD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Parth Patel,Kalpdrum Passi,Chakresh Kumar Jain</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.08652v1</td>\n      <td>Prediction of Cancer Microarray and DNA Methyl...</td>\n      <td>Abstract:  Over the past few years, ther...</td>\n      <td>Quantitative Biology,Quantitative Methods</td>\n      <td>NaN</td>\n      <td>2020-20-20</td>\n      <td>PbLkrR96b86mH4LgQKFrQ9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Danail Stoyanov,Emanuele Colleoni,Philip Edwards</td>\n      <td>arxiv</td>\n      <td>https://arxiv.org/abs/2007.09107v1</td>\n      <td>Synthetic and Real Inputs for ToolSegmentation...</td>\n      <td>Abstract:  Semantic tool segmentation in...</td>\n      <td>Computer Science,Computer Vision And Pattern R...</td>\n      <td>Scene Understanding</td>\n      <td>2020-20-20</td>\n      <td>9VDuxxjMfB8oHUut7rBaWL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = train['authors'].fillna('')+' '+train['title'].fillna('')+' '+train['summary'].fillna('')+' '+train['subjects'].fillna('')","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.index = train['id']\ncorpus.name = 'text'","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm(range(len(corpus)//10000+1)):\n    corpus[10000*i:10000*(i+1)].to_csv('corpus{}.csv'.format(i))","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 20/20 [00:11<00:00,  1.75it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import Field,RawField, TabularDataset","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\nimport os\nimport re\nimport signal\nfrom math import ceil\nfrom os.path import join\nfrom time import time\nimport numpy as np\nimport torch\nfrom numpy.random import choice\nfrom torchtext.data import RawField,Field, TabularDataset\nfrom spacy.lang.en import STOP_WORDS\nimport string\n\nclass DataMaker():\n    def __init__(self, datapath, batch_size, context_size,\n                 num_noise_words=20, max_size=0, num_workers=None):\n        self.datapath=datapath\n        self.batch_size = batch_size\n        self.context_size = context_size\n        self.num_noise_words = num_noise_words\n        self.max_size = max_size\n        self.num_workers = num_workers\n        self.stopwords = list(STOP_WORDS)+list((''.join(string.punctuation)).strip(''))+['-pron-','-PRON-']\n        self.dataset = None\n        \n    def _tokenize_str(self,str_):\n        # keep only alphanumeric and punctations\n        str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n        # remove multiple whitespace characters\n        str_ = re.sub(r'\\s{2,}', ' ', str_)\n        # punctations to tokens\n        str_ = re.sub(r'\\(', ' ( ', str_)\n        str_ = re.sub(r'\\)', ' ) ', str_)\n        str_ = re.sub(r',', ' , ', str_)\n        str_ = re.sub(r'\\.', ' . ', str_)\n        str_ = re.sub(r'!', ' ! ', str_)\n        str_ = re.sub(r'\\?', ' ? ', str_)\n        # split contractions into multiple tokens\n        str_ = re.sub(r'\\'s', ' \\'s', str_)\n        str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n        str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n        str_ = re.sub(r'\\'re', ' \\'re', str_)\n        str_ = re.sub(r'\\'d', ' \\'d', str_)\n        str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n        # lower case\n        return str_.strip().lower().split()\n    \n    def getdata(self):\n        start_time=time()\n        doc_field = RawField()\n        text_field = Field(stop_words=self.stopwords,pad_token=None,tokenize=self._tokenize_str)\n        dataset = TabularDataset(self.datapath,skip_header=True,format='CSV',fields=[('id',doc_field),('text',text_field)])\n        text_field.build_vocab(dataset)\n        self.dataset=dataset\n        print('Time taken --- {} seconds'.format(time()-start_time))\n        return dataset\n    \n    def getgenerator(self):\n        return NCEData(self.dataset,self.batch_size,self.context_size,self.num_noise_words,self.max_size,self.num_workers)\n\n\nclass NCEData(object):\n    \n    # code inspired by parallel generators in https://github.com/fchollet/keras\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, max_size, num_workers):\n        self.max_size = max_size\n\n        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()\n        if self.num_workers is None:\n            self.num_workers = 1\n\n        self._generator = _NCEGenerator(\n            dataset,\n            batch_size,\n            context_size,\n            num_noise_words,\n            _NCEGeneratorState(context_size))\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def __len__(self):\n        return len(self._generator)\n\n    def vocabulary_size(self):\n        return self._generator.vocabulary_size()\n\n    def start(self):\n        \"\"\"Starts num_worker processes that generate batches of data.\"\"\"\n        self._queue = multiprocessing.Queue(maxsize=self.max_size)\n        self._stop_event = multiprocessing.Event()\n\n        for _ in range(self.num_workers):\n            process = multiprocessing.Process(target=self._parallel_task)\n            process.daemon = True\n            self._processes.append(process)\n            process.start()\n\n    def _parallel_task(self):\n        while not self._stop_event.is_set():\n            try:\n                batch = self._generator.next()\n                # queue blocks a call to put() until a free slot is available\n                self._queue.put(batch)\n            except KeyboardInterrupt:\n                self._stop_event.set()\n\n    def get_generator(self):\n        \"\"\"Returns a generator that yields batches of data.\"\"\"\n        while self._is_running():\n            yield self._queue.get()\n\n    def stop(self):\n        \"\"\"Terminates all processes that were created with start().\"\"\"\n        if self._is_running():\n            self._stop_event.set()\n\n        for process in self._processes:\n            if process.is_alive():\n                os.kill(process.pid, signal.SIGINT)\n                process.join()\n\n        if self._queue is not None:\n            self._queue.close()\n\n        self._queue = None\n        self._stop_event = None\n        self._processes = []\n\n    def _is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n\nclass _NCEGenerator(object):\n    \"\"\"An infinite, process-safe batch generator for noise-contrastive\n    estimation of word vector models.\n    Parameters\n    ----------\n    state: paragraphvec.data._NCEGeneratorState\n        Initial (indexing) state of the generator.\n    For other parameters see the NCEData class.\n    \"\"\"\n    def __init__(self, dataset, batch_size, context_size,\n                 num_noise_words, state):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.context_size = context_size\n        self.num_noise_words = num_noise_words\n\n        self._vocabulary = self.dataset.fields['text'].vocab\n        self._sample_noise = None\n        self._init_noise_distribution()\n        self._state = state\n\n    def _init_noise_distribution(self):\n        # we use a unigram distribution raised to the 3/4rd power,\n        # as proposed by T. Mikolov et al. in Distributed Representations\n        # of Words and Phrases and their Compositionality\n        probs = np.zeros(len(self._vocabulary) - 1)\n\n        for word, freq in self._vocabulary.freqs.items():\n            probs[self._word_to_index(word)] = freq\n\n        probs = np.power(probs, 0.75)\n        probs /= np.sum(probs)\n\n        self._sample_noise = lambda: choice(\n            probs.shape[0], self.num_noise_words, p=probs).tolist()\n\n    def __len__(self):\n        num_examples = sum(self._num_examples_in_doc(d) for d in self.dataset)\n        return ceil(num_examples / self.batch_size)\n\n    def vocabulary_size(self):\n        return len(self._vocabulary) - 1\n\n    def next(self):\n        \"\"\"Updates state for the next process in a process-safe manner\n        and generates the current batch.\"\"\"\n        prev_doc_id, prev_in_doc_pos = self._state.update_state(\n            self.dataset,\n            self.batch_size,\n            self.context_size,\n            self._num_examples_in_doc)\n\n        # generate the actual batch\n        batch = _NCEBatch(self.context_size)\n\n        while len(batch) < self.batch_size:\n            if prev_doc_id == len(self.dataset):\n                # last document exhausted\n                batch.torch_()\n                return batch\n            if prev_in_doc_pos <= (len(self.dataset[prev_doc_id].text) - 1\n                                   - self.context_size):\n                # more examples in the current document\n                self._add_example_to_batch(prev_doc_id, prev_in_doc_pos, batch)\n                prev_in_doc_pos += 1\n            else:\n                # go to the next document\n                prev_doc_id += 1\n                prev_in_doc_pos = self.context_size\n\n        batch.torch_()\n        return batch\n\n    def _num_examples_in_doc(self, doc, in_doc_pos=None):\n        if in_doc_pos is not None:\n            # number of remaining\n            if len(doc.text) - in_doc_pos >= self.context_size + 1:\n                return len(doc.text) - in_doc_pos - self.context_size\n            return 0\n\n        if len(doc.text) >= 2 * self.context_size + 1:\n            # total number\n            return len(doc.text) - 2 * self.context_size\n        return 0\n\n    def _add_example_to_batch(self, doc_id, in_doc_pos, batch):\n        doc = self.dataset[doc_id].text\n        batch.doc_ids.append(doc_id)\n\n        # sample from the noise distribution\n        current_noise = self._sample_noise()\n        current_noise.insert(0, self._word_to_index(doc[in_doc_pos]))\n        batch.target_noise_ids.append(current_noise)\n\n        if self.context_size == 0:\n            return\n\n        current_context = []\n        context_indices = (in_doc_pos + diff for diff in\n                           range(-self.context_size, self.context_size + 1)\n                           if diff != 0)\n\n        for i in context_indices:\n            context_id = self._word_to_index(doc[i])\n            current_context.append(context_id)\n        batch.context_ids.append(current_context)\n\n    def _word_to_index(self, word):\n        return self._vocabulary.stoi[word] - 1\n\n\nclass _NCEGeneratorState(object):\n    \"\"\"Batch generator state that is represented with a document id and\n    in-document position. It abstracts a process-safe indexing mechanism.\"\"\"\n    def __init__(self, context_size):\n        # use raw values because both indices have\n        # to manually be locked together\n        self._doc_id = multiprocessing.RawValue('i', 0)\n        self._in_doc_pos = multiprocessing.RawValue('i', context_size)\n        self._lock = multiprocessing.Lock()\n\n    def update_state(self, dataset, batch_size,\n                     context_size, num_examples_in_doc):\n        \"\"\"Returns current indices and computes new indices for the\n        next process.\"\"\"\n        with self._lock:\n            doc_id = self._doc_id.value\n            in_doc_pos = self._in_doc_pos.value\n            self._advance_indices(\n                dataset, batch_size, context_size, num_examples_in_doc)\n            return doc_id, in_doc_pos\n\n    def _advance_indices(self, dataset, batch_size,\n                         context_size, num_examples_in_doc):\n        num_examples = num_examples_in_doc(\n            dataset[self._doc_id.value], self._in_doc_pos.value)\n\n        if num_examples > batch_size:\n            # more examples in the current document\n            self._in_doc_pos.value += batch_size\n            return\n\n        if num_examples == batch_size:\n            # just enough examples in the current document\n            if self._doc_id.value < len(dataset) - 1:\n                self._doc_id.value += 1\n            else:\n                self._doc_id.value = 0\n            self._in_doc_pos.value = context_size\n            return\n\n        while num_examples < batch_size:\n            if self._doc_id.value == len(dataset) - 1:\n                # last document: reset indices\n                self._doc_id.value = 0\n                self._in_doc_pos.value = context_size\n                return\n\n            self._doc_id.value += 1\n            num_examples += num_examples_in_doc(\n                dataset[self._doc_id.value])\n\n        self._in_doc_pos.value = (len(dataset[self._doc_id.value].text)\n                                  - context_size\n                                  - (num_examples - batch_size))\n\n\nclass _NCEBatch(object):\n    def __init__(self, context_size):\n        self.context_ids = [] if context_size > 0 else None\n        self.doc_ids = []\n        self.target_noise_ids = []\n\n    def __len__(self):\n        return len(self.doc_ids)\n\n    def torch_(self):\n        if self.context_ids is not None:\n            self.context_ids = torch.LongTensor(self.context_ids)\n        self.doc_ids = torch.LongTensor(self.doc_ids)\n        self.target_noise_ids = torch.LongTensor(self.target_noise_ids)\n\n    def cuda_(self):\n        if self.context_ids is not None:\n            self.context_ids = self.context_ids.cuda()\n        self.doc_ids = self.doc_ids.cuda()\n        self.target_noise_ids = self.target_noise_ids.cuda()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataMaker('corpus1.csv',batch_size=256,context_size=3,num_workers=-1)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=data.getdata()","execution_count":21,"outputs":[{"output_type":"stream","text":"Time taken --- 3.182295799255371 seconds\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = data.getgenerator()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(generator)","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"3636"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NegativeSampling(nn.Module):\n    \n    \n    def __init__(self):\n        super(NegativeSampling, self).__init__()\n        self._log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, scores):\n        \n        k = scores.size()[1] - 1\n        return -torch.sum(\n            self._log_sigmoid(scores[:, 0])\n            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n        ) / scores.size()[0]","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DM(nn.Module):\n    \"\"\"Distributed Memory version of Paragraph Vectors.\n    Parameters\n    ----------\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n    num_docs: int\n        Number of documents in a dataset.\n    num_words: int\n        Number of distinct words in a daset (i.e. vocabulary size).\n    \"\"\"\n    def __init__(self, vec_dim, num_docs, num_words):\n        super(DM, self).__init__()\n        # paragraph matrix\n        self._D = nn.Parameter(\n            torch.randn(num_docs, vec_dim), requires_grad=True)\n        # word matrix\n        self._W = nn.Parameter(\n            torch.randn(num_words, vec_dim), requires_grad=True)\n        # output layer parameters\n        self._O = nn.Parameter(\n            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n\n    def forward(self, context_ids, doc_ids, target_noise_ids):\n        \"\"\"Sparse computation of scores (unnormalized log probabilities)\n        that should be passed to the negative sampling loss.\n        Parameters\n        ----------\n        context_ids: torch.Tensor of size (batch_size, num_context_words)\n            Vocabulary indices of context words.\n        doc_ids: torch.Tensor of size (batch_size,)\n            Document indices of paragraphs.\n        target_noise_ids: torch.Tensor of size (batch_size, num_noise_words + 1)\n            Vocabulary indices of target and noise words. The first element in\n            each row is the ground truth index (i.e. the target), other\n            elements are indices of samples from the noise distribution.\n        Returns\n        -------\n            autograd.Variable of size (batch_size, num_noise_words + 1)\n        \"\"\"\n        # combine a paragraph vector with word vectors of\n        # input (context) words\n        x = torch.add(\n            self._D[doc_ids, :], torch.sum(self._W[context_ids, :], dim=1))\n\n        # sparse computation of scores (unnormalized log probabilities)\n        # for negative sampling\n        return torch.bmm(\n            x.unsqueeze(1),\n            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n\n    def get_paragraph_vector(self):\n        return self._D.data.tolist()\n    \n    def fit(self,generator,epochs,num_batches):\n        \n        opt=torch.optim.Adam(self.parameters(),lr=0.0001)\n        cost_func = NegativeSampling()\n        if torch.cuda.is_available():            \n            cost_func.cuda()\n\n        generator.start()        \n        \n        batch_maker = generator.get_generator()\n        for epoch in range(epochs):\n            step = 0\n            loss=[]\n            for batch_i in tqdm.tqdm(range(num_batches)):\n#                 step+=1\n                batch = next(batch_maker)\n                if torch.cuda.is_available():\n                    batch.cuda_()\n                \n                x = self.forward(\n                        batch.context_ids,\n                        batch.doc_ids,\n                        batch.target_noise_ids) \n                x = cost_func.forward(x)\n                loss.append(x.item())\n                self.zero_grad()\n                x.backward()\n                opt.step()\n#                 if step%100==0:\n#                     print('-',end='')\n            loss = torch.mean(torch.FloatTensor(loss))\n            print('epoch - {} loss - {:.4f}'.format(epoch+1,loss))\n        generator.stop()\n        \n    def get_embeddings(self,ids):\n        docvecs = self._D.data.tolist()\n        if len(docvecs)!=len(ids):\n            raise(\"Length of ids does'nt match\")\n            return 0\n        self.embeddings = pd.DataFrame({'ids':ids,'vector':docvecs})\n        return self.embeddings\n    \n    def most_similar_docs(self,docs,topk=11):\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        if not isinstance(docs,list):\n            docs = [docs]\n            \n        if not self.embeddings.ids.isin(docs).any():\n            raise('Not in vocab')\n            return 0\n        \n        index = self.embeddings['vector'][self.embeddings['ids'].isin(docs)].values.tolist()\n        index = torch.LongTensor(index).to(device)\n        embeddings = torch.FloatTensor(self.embeddings['vector'].values.tolist()).to(device)\n        similars = self._similarity(index,embeddings,topk)\n        similar_docs = self.embeddings['ids'][similars.indices.tolist()[0]].values.tolist()\n        probs = similars.values.tolist()[0]\n        \n        return similar_docs[1:], probs[1:]\n        \n    def _similarity(self,doc,embeddings,topk):\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        similarity = []\n        \n        cos=nn.CosineSimilarity(dim=0).to(device)\n        for i in doc:\n            inner = []\n            for j in embeddings:\n                inner.append(cos(i.view(-1,1),j.view(-1,1)).tolist())\n            similarity.append(inner)\n        similarity = torch.FloatTensor(similarity).view(1,-1).to(device)\n        return torch.topk(similarity,topk)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DM(vec_dim=100,num_docs=len(dataset),num_words=generator.vocabulary_size())","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.vocabulary_size()","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"107701"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(generator)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"3636"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(generator,num_batches=len(generator),epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getids = pd.read_csv('corpus1.csv')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getids.id","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"0       FcN2HdYAWzGugCtn7FXTy2\n1       5y88idfUmmj5U4FhHoTh8c\n2       ZSHuaNDkcvuHdku6wZMdSj\n3       crnVmbjxaCjvQKEjpkwPtk\n4       N64hmiQPXHzYLp6n6ggAoh\n                 ...          \n9995    CwmwZMzu7uz3QRAUd8VMih\n9996    LxHtTmvFWwyx59ZCxDDG3o\n9997    fxv8pmLHmVYpmXEqsmWBjr\n9998    QMhSYgweKEVSGTREFvaY4L\n9999    MbMjV4atvuWRhKTuLaHjfk\nName: id, Length: 10000, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = model.get_embeddings(getids['id'].values.tolist())","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim = model.most_similar_docs(['LxHtTmvFWwyx59ZCxDDG3o'])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['id'].isin(sim[0]+['LxHtTmvFWwyx59ZCxDDG3o'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}